{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import io\n",
    "import re\n",
    "import code\n",
    "\n",
    "from chainer import no_backprop_mode, cuda, Variable\n",
    "from chainer.functions import relu\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.stats.proportion as prop\n",
    "from exp_settings import settings\n",
    "import trainers.trainer as trainer\n",
    "from measurements.robustness.utils import list_nets\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" # In case the GPU order in the bus is different from the one listed by CUDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_gpu(model, gpu):\n",
    "    \"\"\" Used in case the saved net was in a different GPU from the one used when loading\n",
    "    Args:\n",
    "        model: NNAgent object\n",
    "        gpu: number of the GPU to be transfered to\n",
    "\n",
    "    \"\"\"\n",
    "    for link in range(len(model)):\n",
    "        if hasattr(model[link], 'W'):\n",
    "            model[link].W.to_gpu(gpu)\n",
    "            model[link].ortho_w.to_gpu(gpu)\n",
    "            if hasattr(model[link], \"kernel_size\"):\n",
    "                model[link].mask.to_gpu(gpu)\n",
    "        if hasattr(model[link], 'b') and model[link].b is not None:\n",
    "            model[link].b.to_gpu(gpu)\n",
    "\n",
    "class RenameUnpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        renamed_module = module\n",
    "        return super(RenameUnpickler, self).find_class(renamed_module, name)\n",
    "\n",
    "class RenameUnpickler2(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        renamed_module = module\n",
    "        if module == \"BNN_mod_bu\":\n",
    "            renamed_module = \"BNN_mod_normal\"\n",
    "        return super(RenameUnpickler2, self).find_class(renamed_module, name)\n",
    "\n",
    "def renamed_load(file_obj):\n",
    "    return RenameUnpickler(file_obj).load()\n",
    "\n",
    "def renamed_loads(pickled_bytes):\n",
    "    file_obj = io.BytesIO(pickled_bytes)\n",
    "    return renamed_load(file_obj)\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = renamed_load(fo)\n",
    "    return dict\n",
    "\n",
    "def experiment_routine(**kwargs):\n",
    "    \"\"\" Function to be called to carry multiple trainings under the same settings\n",
    "\n",
    "    Args:\n",
    "        **kwargs:\n",
    "\n",
    "    Returns: kwargs dictionary containing the number of successful and failed trainings\n",
    "\n",
    "    \"\"\"\n",
    "    # Creates string to describe whether the input is normalized across all data or just rescaled to specified interval\n",
    "    if kwargs['normalization']:\n",
    "        in_str = \"norm_in\"\n",
    "    else:\n",
    "        in_str = \"in_int_{}_cent_{}\".format(kwargs['in_interval'], kwargs['in_center'])\n",
    "\n",
    "    arch = kwargs['arch']\n",
    "    date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    curr_dir, curr_fold = os.path.split(os.path.dirname(os.path.realpath(__file__)))\n",
    "\n",
    "    if kwargs['bjorck_config']['iter'] == 0:\n",
    "        dest_dir = curr_dir + \"/trainings/{}/{}/no_ortho/loss_{}/init_{}/{}/{}_tr/{}/\".format(\n",
    "        kwargs['dataset'], arch, kwargs['loss'], kwargs['init'], in_str, kwargs['tr_size'], date)\n",
    "    else:\n",
    "        dest_dir = curr_dir + \"/trainings/{}/{}/ortho/loss_{}/init_{}/{}/{}_tr/{}/\".format(\n",
    "        kwargs['dataset'], arch, kwargs['loss'], kwargs['init'], in_str, kwargs['tr_size'], date)\n",
    "    print(\"Destination folder \\n{}\".format(dest_dir))\n",
    "\n",
    "    j = 0\n",
    "    n_exp = kwargs['n_exp']\n",
    "    fail = 0\n",
    "    success = 0\n",
    "    while success < n_exp:\n",
    "        # Sets the name of the destination folder for current training with the main training settings\n",
    "        exp_name = \"loss_{}_ep_{}_lr_{}_x_var_{}_d_{}_{}\".format(kwargs['loss'], kwargs['n_epoch'], kwargs['lr'], kwargs['x_var'], kwargs['d'], j)\n",
    "        # Changes the number tag of destination folder until a number that wasn'target used\n",
    "        while os.path.exists(dest_dir + exp_name):\n",
    "            j += 1\n",
    "            exp_name = \"loss_{}_ep_{}_lr_{}_x_var_{}_d_{}_{}\".format(kwargs['loss'], kwargs['n_epoch'], kwargs['lr'],  kwargs['x_var'], kwargs['d'], j)\n",
    "\n",
    "        data_save_dir = dest_dir + exp_name\n",
    "        os.makedirs(data_save_dir)\n",
    "\n",
    "        kwargs_train = {'exp_count': success + fail, 'success': success, 'data_save_dir': data_save_dir}\n",
    "        kwargs = {**kwargs, **kwargs_train}\n",
    "        print(\"#### Training {}\".format(success + fail + 1))\n",
    "        print(\"## X_var: {} | d: {} | lr: {} | tr_size: {}\".format(kwargs['x_var'], kwargs['d'], kwargs['lr'], kwargs['tr_size']))\n",
    "        tr_result = trainer.run_training(**kwargs)\n",
    "\n",
    "        if tr_result == 0:\n",
    "            fail += 1\n",
    "        else:\n",
    "            success += 1\n",
    "\n",
    "    print(\"*****************************************************************\")\n",
    "    print(\"The number of fails = {} and successes = {}\".format(fail, success))\n",
    "    print(\"*****************************************************************\")\n",
    "\n",
    "    return kwargs\n",
    "\n",
    "def build_net(**kwargs):\n",
    "    \"\"\"Builds a base agent that will be used to load the trained NN\n",
    "    Make sure that all the architecture settings match\n",
    "    \"\"\"\n",
    "    network = trainer.NNAgent(**kwargs)\n",
    "    network.prepare_data(**kwargs)\n",
    "    network.create_model(**kwargs)\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture used: 4CVHL_8c4k2sx8c3k1sx16c4k2sx16c3k1sx_3FCHL_512x512x512\n"
     ]
    }
   ],
   "source": [
    "mode = \"load\"\n",
    "gpu = 1\n",
    "cp.cuda.Device(gpu).use()     \n",
    "loss = \"zhen\"\n",
    "d = cp.asarray(4.0, dtype=cp.float32)\n",
    "x_var = cp.asarray(0.0001, dtype=cp.float32)\n",
    "\n",
    "input_params = {'loss': loss, 'd': d, 'x_var': x_var, 'gpu': gpu}\n",
    "\n",
    "kwargs = settings(input_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/thomas/sra/measurements/cifar10/cnn/4CVHL_8c4k2sx8c3k1sx16c4k2sx16c3k1sx_3FCHL_512x512x512/\n",
      "### Measurements will be carried for zhen loss, d = 4.0 and x_var = 1e-04\n",
      "['/home/thomas/trained_NNs/cifar10/cnn/4CVHL_8c4k2sx8c3k1sx16c4k2sx16c3k1sx_3FCHL_512x512x512/trained_loss_zhen_ep_300_x_var_1e-04_d_4.0_1', '/home/thomas/trained_NNs/cifar10/cnn/4CVHL_8c4k2sx8c3k1sx16c4k2sx16c3k1sx_3FCHL_512x512x512/trained_loss_zhen_ep_300_x_var_1e-04_d_4.0_3']\n",
      "/home/thomas/trained_NNs/cifar10/cnn/4CVHL_8c4k2sx8c3k1sx16c4k2sx16c3k1sx_3FCHL_512x512x512\n",
      "/home/thomas/sra/measurements/cifar10/cnn/4CVHL_8c4k2sx8c3k1sx16c4k2sx16c3k1sx_3FCHL_512x512x512/\n",
      "/home/thomas/trained_NNs/cifar10/cnn/4CVHL_8c4k2sx8c3k1sx16c4k2sx16c3k1sx_3FCHL_512x512x512/trained_loss_zhen_ep_300_x_var_1e-04_d_4.0_1\n",
      "Using zhen loss\n",
      "#### Preparing dataset\n",
      "Dataset: cifar10 with 50000 training samples\n",
      "Dataset file to be loaded:\n",
      "/home/thomas/datasets/dataset_cifar10/cifar10_preprocessed_data_1.0_0.5.npy\n",
      "#### Finished data preparation\n",
      "############# Number of epochs: 140\n",
      "NN with orthogonal layers\n",
      "### Generating NN model\n",
      "# If this is the first time running it can take several minutes. Please wait\n",
      "14\n",
      "Mean pairwise dot offdiag:  1.9782373e-08\n",
      "Mean pairwise dot diag:  1.0\n",
      "13\n",
      "Mean pairwise dot offdiag:  3.07407e-08\n",
      "Mean pairwise dot diag:  1.0\n",
      "12\n",
      "Mean pairwise dot offdiag:  1.9642655e-05\n",
      "Mean pairwise dot diag:  0.9999571\n",
      "12\n",
      "Mean pairwise dot offdiag:  1.9642655e-05\n",
      "Mean pairwise dot diag:  0.9999571\n",
      "13\n",
      "Mean pairwise dot offdiag:  3.07407e-08\n",
      "Mean pairwise dot diag:  1.0\n",
      "14\n",
      "13\n",
      "Mean pairwise dot offdiag:  1.8532488e-08\n",
      "Mean pairwise dot diag:  1.0\n",
      "12\n",
      "Mean pairwise dot offdiag:  1.9621803e-08\n",
      "Mean pairwise dot diag:  1.0\n",
      "11\n",
      "Mean pairwise dot offdiag:  2.046623e-08\n",
      "Mean pairwise dot diag:  0.9999979\n",
      "11\n",
      "Mean pairwise dot offdiag:  2.046623e-08\n",
      "Mean pairwise dot diag:  0.9999979\n",
      "12\n",
      "Mean pairwise dot offdiag:  1.9621803e-08\n",
      "Mean pairwise dot diag:  1.0\n",
      "13\n",
      "17\n",
      "Mean pairwise dot offdiag:  1.4165588e-08\n",
      "Mean pairwise dot diag:  1.0\n",
      "16\n",
      "Mean pairwise dot offdiag:  1.50195e-08\n",
      "Mean pairwise dot diag:  1.0\n",
      "15\n",
      "Mean pairwise dot offdiag:  4.1201847e-07\n",
      "Mean pairwise dot diag:  0.9999992\n",
      "15\n",
      "Mean pairwise dot offdiag:  4.1201847e-07\n",
      "Mean pairwise dot diag:  0.9999992\n",
      "16\n",
      "Mean pairwise dot offdiag:  1.50195e-08\n",
      "Mean pairwise dot diag:  1.0\n",
      "17\n",
      "15\n",
      "Mean pairwise dot offdiag:  1.5611773e-08\n",
      "Mean pairwise dot diag:  1.0\n",
      "14\n",
      "Mean pairwise dot offdiag:  1.5869208e-08\n",
      "Mean pairwise dot diag:  1.0\n",
      "13\n",
      "Mean pairwise dot offdiag:  1.5812866e-08\n",
      "Mean pairwise dot diag:  0.9999056\n",
      "13\n",
      "Mean pairwise dot offdiag:  1.5812866e-08\n",
      "Mean pairwise dot diag:  0.9999056\n",
      "14\n",
      "Mean pairwise dot offdiag:  1.5869208e-08\n",
      "Mean pairwise dot diag:  1.0\n",
      "15\n",
      "18\n",
      "Mean pairwise dot offdiag:  1.453049e-08\n",
      "Mean pairwise dot diag:  0.99999994\n",
      "17\n",
      "Mean pairwise dot offdiag:  2.7818146e-08\n",
      "Mean pairwise dot diag:  0.99999964\n",
      "16\n",
      "Mean pairwise dot offdiag:  1.1411594e-05\n",
      "Mean pairwise dot diag:  0.9997315\n",
      "16\n",
      "Mean pairwise dot offdiag:  1.1411594e-05\n",
      "Mean pairwise dot diag:  0.9997315\n",
      "17\n",
      "Mean pairwise dot offdiag:  2.7818146e-08\n",
      "Mean pairwise dot diag:  0.99999964\n",
      "18\n",
      "19\n",
      "Mean pairwise dot offdiag:  1.3487406e-08\n",
      "Mean pairwise dot diag:  1.0\n",
      "18\n",
      "Mean pairwise dot offdiag:  8.534166e-08\n",
      "Mean pairwise dot diag:  0.9999995\n",
      "17\n",
      "Mean pairwise dot offdiag:  2.0138103e-05\n",
      "Mean pairwise dot diag:  0.99981886\n",
      "17\n",
      "Mean pairwise dot offdiag:  2.0138103e-05\n",
      "Mean pairwise dot diag:  0.99981886\n",
      "18\n",
      "Mean pairwise dot offdiag:  8.534166e-08\n",
      "Mean pairwise dot diag:  0.9999995\n",
      "19\n",
      "21\n",
      "Mean pairwise dot offdiag:  1.1123241e-08\n",
      "Mean pairwise dot diag:  1.0\n",
      "20\n",
      "Mean pairwise dot offdiag:  2.8881376e-08\n",
      "Mean pairwise dot diag:  0.9999999\n",
      "19\n",
      "Mean pairwise dot offdiag:  5.2753785e-06\n",
      "Mean pairwise dot diag:  0.9999838\n",
      "19\n",
      "Mean pairwise dot offdiag:  5.2753785e-06\n",
      "Mean pairwise dot diag:  0.9999838\n",
      "20\n",
      "Mean pairwise dot offdiag:  2.8881376e-08\n",
      "Mean pairwise dot diag:  0.9999999\n",
      "21\n",
      "13\n",
      "Mean pairwise dot offdiag:  1.9525517e-08\n",
      "Mean pairwise dot diag:  1.0\n",
      "12\n",
      "Mean pairwise dot offdiag:  2.2615009e-08\n",
      "Mean pairwise dot diag:  1.0\n",
      "11\n",
      "Mean pairwise dot offdiag:  1.705575e-06\n",
      "Mean pairwise dot diag:  0.99999535\n",
      "11\n",
      "Mean pairwise dot offdiag:  1.705575e-06\n",
      "Mean pairwise dot diag:  0.99999535\n",
      "12\n",
      "Mean pairwise dot offdiag:  2.2615009e-08\n",
      "Mean pairwise dot diag:  1.0\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "_, curr_fold = os.path.split(kwargs['net_save_dir'])\n",
    "curr_dir = os.getcwd()\n",
    "measures_save_dir = curr_dir + \"/measurements/{}/\".format(kwargs['dataset']) + \"{}/{}/\".format(kwargs['arch'],curr_fold)\n",
    "print(measures_save_dir)\n",
    "if not os.path.isdir(measures_save_dir[0:-1]):\n",
    "    os.makedirs(measures_save_dir[0:-1])\n",
    "\n",
    "curr_dir, curr_fold = os.path.split(curr_dir)\n",
    "filelist = list_nets(kwargs['net_save_dir'], loss = loss, d = d, x_var = x_var, load_mode = mode)\n",
    "print(filelist)\n",
    "# loops through all files in the .csv file\n",
    "file = filelist[0]\n",
    "net_path = file\n",
    "print(kwargs['net_save_dir'])\n",
    "print(measures_save_dir)\n",
    "print(file)\n",
    "kwargs_load = {'exp_count': None, 'success': None, 'data_save_dir': None}\n",
    "kwargs = {**kwargs, **kwargs_load}\n",
    "# Builds NNAgent object with architecture settings\n",
    "network = build_net(**kwargs)\n",
    "# Load trained NN into the NNAgent object\n",
    "network.model = unpickle(net_path)\n",
    "# Changes GPU in case the GPU used during training is different from the one used from here\n",
    "change_gpu(network.model, gpu)\n",
    "# Prepares data with same settings as ones used for training\n",
    "network.prepare_data(mode='load', **kwargs)\n",
    "\n",
    "# Data used for measurements are TEST DATA. Here it changes it to Variable object so that\n",
    "# differentiation w.r.t. inputs can be done\n",
    "x_m = Variable(network.te_x)\n",
    "target = Variable(network.te_y)\n",
    "\n",
    "# Parses the trained NN settings type of loss, x_var, d, training epochs and training number from the file name\n",
    "loss = re.search('loss_(.*)_ep', net_path).group(1)\n",
    "epochs = re.search('_ep_(.*)_x_var', net_path).group(1)\n",
    "x_var = re.search('x_var_(.*)_d', net_path).group(1)\n",
    "d = re.search('{}_d_(.*)_'.format(x_var), net_path).group(1)\n",
    "training_num = float(re.search('_d_{}_(.*)'.format(d), net_path).group(1))\n",
    "x_var = float(x_var)\n",
    "d = float(d)\n",
    "\n",
    "arch = network.model.arch\n",
    "network.loss = loss\n",
    "network.epochs = epochs\n",
    "network.x_var = x_var\n",
    "network.d = d\n",
    "network.model.intvl_in = network.intvl_in\n",
    "network.model.center_in = network.center_in\n",
    "\n",
    "# Sets the Bjorck orthogonalization settings for each layer\n",
    "for j in range(len(network.model)):\n",
    "    if hasattr(network.model[j], 'W'):\n",
    "        network.model[j].config['dynamic_iter'] = True\n",
    "        network.model[j].dynamic_iter = True\n",
    "\n",
    "network.model.ortho_iter_red()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(network.model))\n",
    "layer = 6\n",
    "aux1 = cuda.to_cpu(network.model[layer].ortho_w.array)\n",
    "aux2 = np.linalg.svd(aux1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 12, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "layer = 1\n",
    "aux1 = cuda.to_cpu(network.model[layer].ortho_w.array)\n",
    "print(aux1.shape)\n",
    "aux2 = SingularValues(aux1, (12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(aux2.shape).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chainer import cuda\n",
    "import pandas\n",
    "import pingouin\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = 0.5\n",
    "samples = 50000\n",
    "out_samples = cuda.to_cpu(network.model.output_sampling(x_m, 0, 1))\n",
    "class_out = np.argmax(out_samples, axis = 1)\n",
    "tgt = cuda.to_cpu(target.array)\n",
    "corr_in = x_m[class_out==tgt]\n",
    "in_samples = np.random.randint(0,corr_in.shape[0],100)\n",
    "histogram = np.zeros(100)\n",
    "t_arr = []\n",
    "r_arr = []\n",
    "for i in in_samples:\n",
    "\tout_samples = network.model.output_sampling(corr_in[i], std, samples)\n",
    "\tavg_out = out_samples.mean(axis = 0)\n",
    "\tdf = pandas.DataFrame(data=cuda.to_cpu(out_samples), index=np.arange(samples), columns=np.arange(10))\n",
    "\t# table = pingouin.pairwise_corr(df)\n",
    "\ttable = pingouin.rcorr(df)\n",
    "\t# histogram += np.histogram(table['r'], bins = 100, range=(-1,1))[0]\n",
    "\tr_arr.append(table)\t\n",
    "\t# r_arr.append(table['r'])\n",
    "\t# t_arr.append(table['r'] * np.sqrt((samples-2)/(1 - table['r']**2)))\n",
    "\t# cov = \n",
    " \t# normal_samples = np.random.multivariate_normal(mean, cov, size=None, check_valid='warn', tol=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0      1       2       3       4       5       6      7      8    9\n",
      "0       -    ***     ***     ***     ***     ***     ***    ***    ***  ***\n",
      "1   0.373      -     ***     ***     ***     ***     ***    ***    ***  ***\n",
      "2   0.623   0.31       -     ***     ***     ***     ***    ***    ***  ***\n",
      "3   0.913  0.457   0.837       -     ***     ***     ***    ***    ***  ***\n",
      "4   0.895   0.45   0.803    0.96       -     ***     ***    ***    ***  ***\n",
      "5   0.887  0.251    0.77   0.913     0.9       -     ***    ***    ***  ***\n",
      "6  -0.411  -0.56  -0.188  -0.356  -0.438  -0.272       -    ***    ***  ***\n",
      "7   0.447  0.111   0.344   0.358   0.361   0.386  -0.505      -    ***  ***\n",
      "8   0.968  0.447   0.719   0.948    0.92   0.908  -0.348  0.426      -  ***\n",
      "9   0.419  0.098   0.178   0.311   0.371   0.218  -0.561   0.08  0.313    -\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(r_arr[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00152423 -0.00107941 -0.00136318 -0.00131011 -0.0017362  -0.00116517\n",
      " -0.00108493  0.00141044 -0.00126001 -0.00157429]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "out_samples = network.model.output_sampling(corr_in[i], std, samples)\n",
    "print(out_samples[0,:])\n",
    "print(tgt[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, mean_s, var_s, mean_h, var_h = network.model.moment_propagation(len(network.model)-1,corr_in, std**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6195,)\n"
     ]
    }
   ],
   "source": [
    "# var_s.shape\n",
    "corr_in_idx = np.arange(10000)[class_out==tgt]\n",
    "print(corr_in_idx.shape)\n",
    "# type(var_s[in_samples[0]])\n",
    "# out_cov = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.9999994e-01  2.8501212e-08  1.7293511e-08 ... -7.1759398e-09\n",
      "  -2.8865680e-08 -1.7128910e-08]\n",
      " [ 2.8501212e-08  9.9999988e-01  5.5824763e-09 ... -4.9563535e-09\n",
      "   2.7733185e-08 -2.2256419e-09]\n",
      " [ 1.7293511e-08  5.5824763e-09  1.0000000e+00 ...  1.1144550e-08\n",
      "   9.0947720e-09 -7.8051308e-09]\n",
      " ...\n",
      " [-7.1759398e-09 -4.9563535e-09  1.1144550e-08 ...  1.0000001e+00\n",
      "  -5.0594764e-09  1.5275738e-08]\n",
      " [-2.8865680e-08  2.7733185e-08  9.0947720e-09 ... -5.0594764e-09\n",
      "   9.9999952e-01  9.7132942e-09]\n",
      " [-1.7128910e-08 -2.2256419e-09 -7.8051308e-09 ...  1.5275738e-08\n",
      "   9.7132942e-09  1.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "a = network.model[7].ortho_w.array @ network.model[7].ortho_w.array.T\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# off_diag = np.logical_not(np.eye(10, dtype = 'bool'))\n",
    "# corr_out_arr = corr_out[off_diag]\n",
    "cov_out = network.model[len(network.model)-1].W.array @ cp.diag(var_s[in_samples[0]].array) @ network.model[len(network.model)-1].W.array.T\n",
    "corr_out = (cp.diag(1/cp.sqrt(cp.diag(cov_out)))) @ cov_out @ (cp.diag(1/cp.sqrt(cp.diag(cov_out))))\n",
    "corr_out_arr = corr_out[np.tril(corr_out, -1) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.asarray(r_arr)\n",
    "t = np.asarray(t_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.81231212,  0.93257026,  0.827628  ,  0.86840622,  0.86460158,\n",
       "        0.46438711,  0.84419593,  0.83794323, -0.58857432,  0.81958599,\n",
       "        0.85126954,  0.8941648 ,  0.84215664,  0.66383754,  0.81617388,\n",
       "        0.81821718, -0.83822812,  0.9629181 ,  0.9674385 ,  0.98157067,\n",
       "        0.72902316,  0.97442257,  0.97286723, -0.77811047,  0.9890172 ,\n",
       "        0.99435307,  0.87756841,  0.98867852,  0.99482291, -0.90548836,\n",
       "        0.98615401,  0.82775242,  0.9762277 ,  0.98278516, -0.88206703,\n",
       "        0.83789781,  0.99388445,  0.99389647, -0.8731903 ,  0.84552913,\n",
       "        0.85687563, -0.94545084,  0.98928982, -0.87399845, -0.86898948])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 320x240 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fig,ax = plt.subplots(1)\n",
    "idx_aux = np.random.randint(0,100,1)[0]\n",
    "idx = corr_in_idx[in_samples[idx_aux]]\n",
    "cov_out = network.model[len(network.model)-1].W.array @ cp.diag(var_s[in_samples[idx_aux]].array) @ network.model[len(network.model)-1].W.array.T\n",
    "corr_out = (cp.diag(1/cp.sqrt(cp.diag(cov_out)))) @ cov_out @ (cp.diag(1/cp.sqrt(cp.diag(cov_out))))\n",
    "# statsmodels.stats.moment_helpers.cov2corr(cuda.to_cpu(corr_out))\n",
    "corr_out_arr = corr_out[np.tril(corr_out, -1) != 0]\n",
    "# print(r[idx_aux])\n",
    "# print(cuda.to_cpu(corr_out_arr).shape)\n",
    "fig = plt.figure(dpi=50,constrained_layout=True)\n",
    "ax = fig.add_subplot(111)\n",
    "# plt.xlim([-0.012, -0.008])\n",
    "# plt.ylim([-0.0135, -0.011])\n",
    "x_axis = np.arange(-1, 1, 0.02)\n",
    "# line1 = ax.plot(x_axis, histogram)\n",
    "line1 = ax.hist(r[idx_aux], bins=30, range=(-1,1))\n",
    "# line2 = ax.hist(cuda.to_cpu(corr_out_arr), bins=30, range=(-1,1))\n",
    "# plt.plot(out_samples[:,0],out_samples[:,6])\n",
    "# fig.subplots_adjust(left=0,right=1,bottom=0,top=1)\n",
    "# ax.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sra-env-bu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
